{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     dataframe \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframe\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m---> 20\u001b[0m \u001b[43mgoodreads_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36mgoodreads_df\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39magg(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m dataframe\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\blinkist\\lib\\site-packages\\pandas\\core\\generic.py:5583\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5577\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5579\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5580\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5581\u001b[0m ):\n\u001b[0;32m   5582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'url'"
     ]
    }
   ],
   "source": [
    "def goodreads_df():  #goodreadsList as argument\n",
    "\n",
    "\n",
    "    #os.system(\"scrapy crawl GoodReads -o c.json\")\n",
    "\n",
    "  \n",
    "    file = open(\"d.json\")\n",
    "\n",
    "    data = json.load(file) #data is a list of dictionaries, where key=url, values = list of other versions name for the book\n",
    "\n",
    "    dataframe = pd.DataFrame(data)\n",
    "    \n",
    "    dataframe = dataframe.groupby([\"url\"]).agg('sum')\n",
    "    \n",
    "\n",
    "    dataframe = dataframe.set_index('title')\n",
    "    return dataframe.url\n",
    "\n",
    "\n",
    "goodreads_df()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean goodreads dataframe\n",
    "\n",
    "Find all the strings that are like (Kindle), (Handcover).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have to be improved, probably to something like:\n",
    "#### \"check if this substring is contained in this other string and if the difference is only something inside paranthesis at the end, then that's ok, else is false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " ['(ebook)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(ebook)'],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ['(Paperback)', '(Kindle Edition)', '(ebook)', '(ebook)'],\n",
       " ['(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Audio CD)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(eBook Kindle)',\n",
       "  '(Paperback)',\n",
       "  '(Audio CD)',\n",
       "  '(Audio CD)',\n",
       "  '(Hardcover)',\n",
       "  '(Audible Audio)',\n",
       "  '(Audio CD)',\n",
       "  '(Audiobook)',\n",
       "  '(Audio CD)',\n",
       "  '(Audio CD)'],\n",
       " '',\n",
       " ['(Paperback)'],\n",
       " ['(Kindle Edition)'],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ['(Kindle Edition)', '(Kindle Edition)', '(Paperback)', '(Paperback)'],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ['(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Audio CD)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Audiobook)',\n",
       "  '(Hardcover)',\n",
       "  '(Audio CD)',\n",
       "  '(Audio CD)',\n",
       "  '(ebook)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)'],\n",
       " '',\n",
       " ['(ebook)'],\n",
       " '',\n",
       " ['(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Audible Audio)',\n",
       "  '(Audio CD)',\n",
       "  '(Audio CD)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(ebook)',\n",
       "  '(Hardcover)',\n",
       "  '(Hardcover)',\n",
       "  '(Hardcover)'],\n",
       " ['(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Audiobook)',\n",
       "  '(ebook)'],\n",
       " '',\n",
       " ['(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Audible Audio)',\n",
       "  '(Paperback)',\n",
       "  '(mitp Professional)',\n",
       "  '(Hardcover)',\n",
       "  '(Unknown Binding)'],\n",
       " ['(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(ebook)',\n",
       "  '(Audiobook)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Library Binding)',\n",
       "  '(Audiobook)',\n",
       "  '(Paperback)',\n",
       "  '(Audible Audio)',\n",
       "  '(Audiobook)',\n",
       "  '(Audiobook)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(ebook)',\n",
       "  '(Hardcover)',\n",
       "  '(Unknown Binding)'],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ['(Paperback)', '(Kindle Edition)', '(Kindle Edition)', '(ebook)', '(ebook)'],\n",
       " ['(Paperback)', '(Kindle Edition)', '(ebook)', '(ebook)'],\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " ['(Paperback)', '(Paperback)'],\n",
       " ['(Paperback)',\n",
       "  '(Hardcover)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Hardcover)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Kindle Edition)',\n",
       "  '(Paperback)',\n",
       "  '(Hardcover)'],\n",
       " ['(Paperback)', '(Hardcover)']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #----------------\n",
    "    #Clean Data: \n",
    "    #-------------------------------\n",
    "\n",
    "    def clean2(title):\n",
    "        try:\n",
    "            output = [re.split(r\"(\\([a-z-A-Z\\s]*?\\))$\",x)[1].strip() for x in title] #regex for finding paranthesis(and what is inside) at the ending of the string\n",
    "            return output\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "clean = lambda lista : [re.split(r\"(\\([a-z-A-Z\\s]*?\\))$\",x)[0].strip() for x in lista]\n",
    "    \n",
    "dataframe = goodreads_df()\n",
    "dataframe[\"otherEditions\"] = dataframe[\"otherEditions\"].apply(clean2)\n",
    "\n",
    "dirt = [line for line in dataframe[\"otherEditions\"].values]\n",
    "\n",
    "dirt\n",
    "    \n",
    "    #-----------------------\n",
    "    #-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blinkist sitemap dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>books</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"I have a dream\"</td>\n",
       "      <td>www.blinkist.com/en/books/i-have-a-dream-de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>#Education For Future</td>\n",
       "      <td>www.blinkist.com/en/books/number-education-for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>#GIRLBOSS</td>\n",
       "      <td>www.blinkist.com/en/books/number-girlboss-en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>#NeverAgain</td>\n",
       "      <td>www.blinkist.com/en/books/number-neveragain-en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#Upcycle Your Job</td>\n",
       "      <td>www.blinkist.com/en/books/number-upcycle-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>5199</td>\n",
       "      <td>Überleben unter Kollegen</td>\n",
       "      <td>www.blinkist.com/en/books/uberleben-unter-koll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>5200</td>\n",
       "      <td>Überreichtum</td>\n",
       "      <td>www.blinkist.com/en/books/uberreichtum-de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5201</th>\n",
       "      <td>5201</td>\n",
       "      <td>Überwachen und Strafen</td>\n",
       "      <td>www.blinkist.com/en/books/uberwachen-und-straf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5202</th>\n",
       "      <td>5202</td>\n",
       "      <td>Überzeugen wie Steve Jobs</td>\n",
       "      <td>www.blinkist.com/en/books/uberzeugen-wie-steve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5203</th>\n",
       "      <td>5203</td>\n",
       "      <td>Überzeugt!</td>\n",
       "      <td>www.blinkist.com/en/books/uberzeugt-de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5204 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                      books  \\\n",
       "0              0           \"I have a dream\"   \n",
       "1              1      #Education For Future   \n",
       "2              2                  #GIRLBOSS   \n",
       "3              3                #NeverAgain   \n",
       "4              4          #Upcycle Your Job   \n",
       "...          ...                        ...   \n",
       "5199        5199   Überleben unter Kollegen   \n",
       "5200        5200               Überreichtum   \n",
       "5201        5201     Überwachen und Strafen   \n",
       "5202        5202  Überzeugen wie Steve Jobs   \n",
       "5203        5203                 Überzeugt!   \n",
       "\n",
       "                                                   urls  \n",
       "0           www.blinkist.com/en/books/i-have-a-dream-de  \n",
       "1     www.blinkist.com/en/books/number-education-for...  \n",
       "2          www.blinkist.com/en/books/number-girlboss-en  \n",
       "3        www.blinkist.com/en/books/number-neveragain-en  \n",
       "4     www.blinkist.com/en/books/number-upcycle-your-...  \n",
       "...                                                 ...  \n",
       "5199  www.blinkist.com/en/books/uberleben-unter-koll...  \n",
       "5200          www.blinkist.com/en/books/uberreichtum-de  \n",
       "5201  www.blinkist.com/en/books/uberwachen-und-straf...  \n",
       "5202  www.blinkist.com/en/books/uberzeugen-wie-steve...  \n",
       "5203             www.blinkist.com/en/books/uberzeugt-de  \n",
       "\n",
       "[5204 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blink_df = pd.read_csv(\"blinkist_book_db.csv\")\n",
    "blink_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximize numpy printoption and check for intersection between two dataframes book titles columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['On Intelligence'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "blink_df['books'].values\n",
    "\n",
    "\n",
    "np.intersect1d(blink_df['books'].values, dataframe['title'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Without checking for the other editions columns, the result is very poor. So, this is the next thing that I should do. I am afraid about the cost of this operation, hopefully I can do in a fast way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "504e74f669aa75292b5759c8a8abd751a6a9f2fa10f5687ede16c31d22fd1de6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
